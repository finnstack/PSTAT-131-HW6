---
title: "PSTAT 131 Homework #6"
author: "Finn Stack"
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: show
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

## Tree-Based Models

For this assignment, we will continue working with the file `"pokemon.csv"`, found in `/data`. The file is from Kaggle: <https://www.kaggle.com/abcsds/pokemon>.

The [Pokémon](https://www.pokemon.com/us/) franchise encompasses video games, TV shows, movies, books, and a card game. This data set was drawn from the video game series and contains statistics about 721 Pokémon, or "pocket monsters." In Pokémon games, the user plays as a trainer who collects, trades, and battles Pokémon to (a) collect all the Pokémon and (b) become the champion Pokémon trainer.

Each Pokémon has a [primary type](https://bulbapedia.bulbagarden.net/wiki/Type) (some even have secondary types). Based on their type, a Pokémon is strong against some types, and vulnerable to others. (Think rock, paper, scissors.) A Fire-type Pokémon, for example, is vulnerable to Water-type Pokémon, but strong against Grass-type.

![Fig 1. Houndoom, a Dark/Fire-type canine Pokémon from Generation II.](/Users/finnianstack/Desktop/homework-6-2/images/houndoom.jpg){width="200"}

The goal of this assignment is to build a statistical learning model that can predict the **primary type** of a Pokémon based on its generation, legendary status, and six battle statistics.

**Note: Fitting ensemble tree-based models can take a little while to run. Consider running your models outside of the .Rmd, storing the results, and loading them in your .Rmd to minimize time to knit.**

```{r, echo=FALSE, include=FALSE}

library(tidyverse)
library(tinytex)
library(ranger)
library(tidymodels)
library(ggplot2)
library(tune)
library(glmnet)
library(yardstick)
library(rsample)
library(kernlab)
library(dplyr)
library(discrim)
library(workflowsets)
library(workflows)
library(caret)
library(janitor)
library(corrplot)
library(corrr)
library(ISLR)
library(rpart.plot)
library(vip)
library(xgboost)
tidymodels_prefer()
```

### Exercise 1

Read in the data and set things up as in Homework 5:

```{r}
pokemon <- read.csv("/Users/finnianstack/Desktop/School/PSTAT/PSTAT 131/homework-6/data/Pokemon.csv")
```

- Use `clean_names()`
- Filter out the rarer Pokémon types
- Convert `type_1` and `legendary` to factors

```{r}
library(janitor)
pokemon1 <- clean_names(pokemon)

pokemon_factor <- pokemon1 %>%
  filter(type_1 == 'Bug' | 
           type_1 == 'Fire' | 
           type_1 == 'Grass' |
           type_1 == 'Normal' | 
           type_1 == 'Water' | 
           type_1 == 'Psychic')

pokemon_factor$type_1 <- as.factor(pokemon_factor$type_1)
pokemon_factor$legendary <- as.factor(pokemon_factor$legendary)
pokemon_factor$generation <- as.factor(pokemon_factor$generation)

```

Do an initial split of the data; you can choose the percentage for splitting. Stratify on the outcome variable.

```{r}
set.seed(3435)

pokemon_split <- initial_split(pokemon_factor, prop = 0.75, strata = "type_1")
pokemon_train <- training(pokemon_split)
pokemon_test <- testing(pokemon_split)
```

Fold the training set using *v*-fold cross-validation, with `v = 5`. Stratify on the outcome variable.

```{r}
pokemon_folds <- vfold_cv(pokemon_train, v = 5, strata = type_1)
```

Set up a recipe to predict `type_1` with `legendary`, `generation`, `sp_atk`, `attack`, `speed`, `defense`, `hp`, and `sp_def`:

- Dummy-code `legendary` and `generation`;
- Center and scale all predictors.

```{r}
pokemon_recipe <- recipe(type_1 ~ legendary + generation + sp_atk + attack + 
                        speed + defense + hp + sp_def, data=pokemon_train) %>%
  step_dummy(c(legendary, generation)) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())
```

### Exercise 2

Create a correlation matrix of the training set, using the `corrplot` package. *Note: You can choose how to handle the continuous variables for this plot; justify your decision(s).*

```{r}
pokemon_train %>% 
  select(is.numeric) %>% 
  cor(use = "complete.obs") %>% 
  corrplot(method = "number", type = "lower")
```

What relationships, if any, do you notice? Do these relationships make sense to you?

The correlations between continuous variables are represented by the coefficients in the matrix,
with color coordination to signify the strength of the correlation.

Looking at our matrix, we can see "total" is the most strongly correlated with the other variables. This would make sense because it is the sum of all other variables statistics and is computed directly from them. 

We can also see that "sp_atk" and "sp_def" are pretty positively correlated. This could make sense given both traits involve a sort of base modifier. There is also a relatively strong positive correlation between "defense" and "sp_def". This would make sense as they both involve base damage resistance. "Defense" is the resistance against normal attacks and "sp_def" is the resistance against special attacks. 

### Exercise 3

First, set up a decision tree model and workflow. Tune the `cost_complexity` hyperparameter. Use the same levels we used in Lab 7 -- that is, `range = c(-3, -1)`. Specify that the metric we want to optimize is `roc_auc`. 

Print an `autoplot()` of the results. What do you observe? Does a single decision tree perform better with a smaller or larger complexity penalty?

```{r}
tree_spec <- decision_tree() %>%
  set_engine("rpart")
```

```{r}
class_tree_spec <- tree_spec %>%
  set_mode("classification")
```

```{r}
tree_workflow <- workflow() %>%
  add_model(class_tree_spec %>% 
              set_args(cost_complexity = tune())) %>%
  add_recipe(pokemon_recipe)
```

```{r}
param_grid <- grid_regular(cost_complexity(range = c(-3,-1)), levels = 10)

tune_res <- tune_grid(
  tree_workflow,
  resamples = pokemon_folds,
  grid = param_grid,
  metrics = metric_set(roc_auc))
```

```{r}
autoplot(tune_res)
```

It appears that our roc_auc is pretty high at relatively low paremeters. It peaks around 0.03 at which it starts dropping pretty quickly. 
After this point we can see that is struggles to distinguish the types.
It would seem that a single decision tree would perform better.

### Exercise 4

What is the `roc_auc` of your best-performing pruned decision tree on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

```{r}
tuned_metrics <- collect_metrics(tune_res)
ordered_metrics <- arrange(tuned_metrics, increasing = FALSE)
```

```{r}
head(ordered_metrics, 1)
```
The best roc_auc for the best-performing pruned decision tree is 0.633

### Exercise 5

Using `rpart.plot`, fit and visualize your best-performing pruned decision tree with the *training* set.

```{r}
best_perform <- select_best(tune_res)
final_workflow <- finalize_workflow(tree_workflow, best_perform)
fitted_tree <- fit(final_workflow, data = pokemon_train)
fitted_tree %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE)
```

### Exercise 5

Now set up a random forest model and workflow. Use the `ranger` engine and set `importance = "impurity"`. Tune `mtry`, `trees`, and `min_n`. Using the documentation for `rand_forest()`, explain in your own words what each of these hyperparameters represent.

```{r}
random_forest <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

random_forest_workflow <- workflow() %>%
  add_model(random_forest %>% set_args(mtry = tune(),
                            trees = tune(),
                            min_n = tune())) %>%
  add_recipe(pokemon_recipe)
```

mtry is the umber of variables randomly sampled as candidates at each split. ntrees is the amount of trees that will be created. min_n is the minimum number of data points in a node before creating a new split.

Create a regular grid with 8 levels each. You can choose plausible ranges for each hyperparameter. Note that `mtry` should not be smaller than 1 or larger than 8. **Explain why not. What type of model would `mtry = 8` represent?**

```{r} 
eight_level_grid <- grid_regular(mtry(range = c(1, 7)), 
                                            trees(range = c(64, 128)),
                                            min_n(range = c(30, 150)), 
                                 levels = 8)
```

mtry should be within 1 and 7 because if it equaled it, it would just be taking all of the predictors instead of a random sample. If it were less than 1, it wouldn't have any predictors.

### Exercise 6

Specify `roc_auc` as a metric. Tune the model and print an `autoplot()` of the results. What do you observe? What values of the hyperparameters seem to yield the best performance?

```{r}
tuned_forest <- tune_grid(random_forest_workflow, 
                          resamples = pokemon_folds, 
                          grid = eight_level_grid, 
                          metrics = metric_set(roc_auc))
```

```{r}
autoplot(tuned_forest)
```

### Exercise 7

What is the `roc_auc` of your best-performing random forest model on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

```{r}
collect_metrics(tuned_forest) %>% arrange(desc(mean))

best_random_forest <- select_best(tuned_forest)
```
Our best roc_auc is 0.745 at 82 trees

### Exercise 8

Create a variable importance plot, using `vip()`, with your best-performing random forest model fit on the *training* set.

```{r}
random_forest_final <- finalize_workflow(random_forest_workflow, best_random_forest)
random_forest_final_fit <- fit(random_forest_final, pokemon_train)
random_forest_final_fit %>%
  extract_fit_parsnip() %>% 
  vip()
```

Which variables were most useful? Which were least useful? Are these results what you expected, or not?

Based off of the plot, we can see that special attack was by far the most useful, with speed as the second best.

The least useful variables were legendary and generation.

I think this is what I expected as the generation and legendary variable don't have an influence on type, whereas special attack and speed are much more intertwined with the type.

### Exercise 9

Finally, set up a boosted tree model and workflow. Use the `xgboost` engine. Tune `trees`. Create a regular grid with 10 levels; let `trees` range from 10 to 2000. Specify `roc_auc` and again print an `autoplot()` of the results. 

```{r}
boost <- boost_tree(trees = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")
```

```{r}
boost_workflow <- workflow() %>%
  add_recipe(pokemon_recipe) %>%
  add_model(boost)

```


```{r}
boost_grid <- grid_regular(trees(range = c(10, 2000)), levels = 10)

tune_boost <- tune_grid(
  boost_workflow, 
  resamples = pokemon_folds, 
  grid = boost_grid, 
  metrics = metric_set(roc_auc))
```

```{r}
autoplot(tune_boost)
```

What do you observe?

The roc appears to be generally increasing as the number of trees increase. 


What is the `roc_auc` of your best-performing boosted tree model on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

```{r}
collect_metrics(tune_boost) %>% arrange(desc(mean))
```

The best roc_auc is 0.724 with 231 trees.

### Exercise 10

Display a table of the three ROC AUC values for your best-performing pruned tree, random forest, and boosted tree models. Which performed best on the folds? Select the best of the three and use `select_best()`, `finalize_workflow()`, and `fit()` to fit it to the *testing* set. 

```{r}
best_boost_tree <- select_best(tune_boost)
boost_tree_final <- finalize_workflow(boost_workflow, best_boost_tree)
boost_tree_fit <- fit(boost_tree_final, data = pokemon_train)
```

```{r}
final_class = augment(fitted_tree, new_data = pokemon_train)
final_random = augment(random_forest_final_fit, new_data = pokemon_train)
final_boost = augment(boost_tree_fit, new_data = pokemon_train)
```

```{r}
bind_rows(
  roc_auc(final_class, truth = type_1, .pred_Bug, .pred_Fire, .pred_Grass, 
          .pred_Normal, .pred_Water, .pred_Psychic),
  roc_auc(final_random, truth = type_1, .pred_Bug, .pred_Fire, .pred_Grass, 
          .pred_Normal, .pred_Water, .pred_Psychic),
  roc_auc(final_boost, truth = type_1, .pred_Bug, .pred_Fire, .pred_Grass, 
          .pred_Normal, .pred_Water, .pred_Psychic) 
)

```

The best model is a boost model with 0.811

Print the AUC value of your best-performing model on the testing set. Print the ROC curves. Finally, create and visualize a confusion matrix heat map.

```{r}
final_boost_test = augment(boost_tree_fit, new_data = pokemon_test)

roc_auc(final_boost_test, truth = type_1, .pred_Bug, .pred_Fire, .pred_Grass, 
        .pred_Normal, .pred_Water, .pred_Psychic)
```

```{r}
 autoplot(roc_curve(final_boost_test, truth = type_1, .pred_Bug, .pred_Fire, 
                    .pred_Grass, .pred_Normal, .pred_Water, .pred_Psychic))
```

```{r}
conf_mat(final_boost_test, truth = type_1, estimate = .pred_class) %>% 
  #calclate confusion matri 
  autoplot(type = "heatmap")
```
Which classes was your model most accurate at predicting? Which was it worst at?

It appears that the most accurate model was Bug and Normal, while the least accurate were water and psychic.

## For 231 Students

### Exercise 11

Using the `abalone.txt` data from previous assignments, fit and tune a random forest model to predict `age`. Use stratified cross-validation and select ranges for `mtry`, `min_n`, and `trees`. Present your results. What was the model's RMSE on your testing set?